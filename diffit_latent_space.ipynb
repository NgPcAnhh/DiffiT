{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "# Latent DiffiT: Transformer-Based Diffusion Model for Image Generation\n",
        "\n",
        "This notebook implements a complete Latent DiffiT pipeline for training and generating images with a transformer-based diffusion model.\n",
        "\n",
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from diffusers import AutoencoderKL\n",
        "from einops import rearrange\n",
        "\n",
        "import os\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 1. Mount Google Drive and Set Up Directories\n",
        "\n",
        "We'll mount Google Drive to access our dataset and save generated images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up data and output directories\n",
        "DATA_DIR = \"/content/drive/MyDrive/DiffiT_latent_space/image-net-256/archive/data\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/DiffiT_latent_space/generate_image\"\n",
        "MODEL_DIR = \"/content/drive/MyDrive/DiffiT_latent_space/models\"\n",
        "\n",
        "# Create output and model directories if they don't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# Check the dataset structure\n",
        "print(f\"Dataset directory: {DATA_DIR}\")\n",
        "if os.path.exists(DATA_DIR):\n",
        "    print(f\"Number of class folders: {len(os.listdir(DATA_DIR))}\")\n",
        "    print(f\"First 10 classes: {os.listdir(DATA_DIR)[:10]}\")\n",
        "else:\n",
        "    print(\"Dataset directory not found! Please check the path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 2. Load VAE Model\n",
        "\n",
        "We'll use a pre-trained VAE from Stable Diffusion for encoding images to latent space and decoding from latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Load VAE from Stable Diffusion\n",
        "vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2-base\", subfolder=\"vae\")\n",
        "vae.to(device)\n",
        "vae.eval()  # Set to inference mode\n",
        "\n",
        "# Freeze VAE weights\n",
        "for param in vae.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Get the scaling factor (typically 0.18215 for Stable Diffusion VAE)\n",
        "scaling_factor = getattr(vae.config, \"scaling_factor\", 0.18215)\n",
        "print(f\"VAE scaling factor: {scaling_factor}\")\n",
        "\n",
        "# Define helper functions for encoding and decoding\n",
        "def encode_to_latent(images):\n",
        "    \"\"\"\n",
        "    Encode images to latent space using the VAE.\n",
        "    \n",
        "    Args:\n",
        "        images: Tensor of shape [B, C, H, W] in range [-1, 1]\n",
        "        \n",
        "    Returns:\n",
        "        latents: Tensor of shape [B, 4, H/8, W/8]\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        latent_dist = vae.encode(images).latent_dist\n",
        "        latents = latent_dist.sample()\n",
        "        latents = latents / scaling_factor\n",
        "    return latents\n",
        "\n",
        "def decode_from_latent(latents):\n",
        "    \"\"\"\n",
        "    Decode latents to images using the VAE.\n",
        "    \n",
        "    Args:\n",
        "        latents: Tensor of shape [B, 4, H/8, W/8]\n",
        "        \n",
        "    Returns:\n",
        "        images: Tensor of shape [B, 3, H, W] in range [-1, 1]\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        latents_scaled = latents * scaling_factor\n",
        "        images = vae.decode(latents_scaled).sample\n",
        "    return images\n",
        "\n",
        "# Test VAE with a random image\n",
        "test_image = torch.randn(1, 3, 256, 256).to(device)  # Random image\n",
        "test_latent = encode_to_latent(test_image)\n",
        "test_recon = decode_from_latent(test_latent)\n",
        "\n",
        "print(f\"Test image shape: {test_image.shape}\")\n",
        "print(f\"Test latent shape: {test_latent.shape}\")\n",
        "print(f\"Test reconstruction shape: {test_recon.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 3. Data Loading\n",
        "\n",
        "Set up the ImageNet dataset and dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "def get_imagenet_dataloader(data_dir, batch_size=32, num_workers=2):\n",
        "    # Define transforms for images\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),  # Ensure images are 256x256\n",
        "        transforms.ToTensor(),          # Convert to tensor [0, 1]\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
        "    ])\n",
        "\n",
        "    # Load dataset using ImageFolder\n",
        "    dataset = torchvision.datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return dataloader, dataset\n",
        "\n",
        "# Set up the dataloader\n",
        "batch_size = 32\n",
        "dataloader, dataset = get_imagenet_dataloader(DATA_DIR, batch_size=batch_size)\n",
        "num_classes = len(dataset.classes)\n",
        "\n",
        "print(f\"Dataset loaded with {len(dataset)} images in {num_classes} classes\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "\n",
        "# Display some sample images\n",
        "def show_batch(dataloader):\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images = (images + 1) / 2  # Convert from [-1, 1] to [0, 1] for display\n",
        "    grid = vutils.make_grid(images[:16], nrow=4, padding=2, normalize=False)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    class_names = [dataset.classes[label] for label in labels[:16]]\n",
        "    print(\"Classes:\", class_names)\n",
        "\n",
        "try:\n",
        "    show_batch(dataloader)\n",
        "except Exception as e:\n",
        "    print(f\"Error displaying batch: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 4. Latent DiffiT Model Implementation\n",
        "\n",
        "Implement the transformer-based diffusion model for latent space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# 4.1 Time Embedding and Utilities\n",
        "\n",
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    \"\"\"Sinusoidal position embeddings for timesteps\"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    \"\"\"Swish activation function: x * sigmoid(x)\"\"\"\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    \"\"\"Time embedding with MLP and Swish activation\"\"\"\n",
        "    def __init__(self, time_embed_dim, model_dim):\n",
        "        super().__init__()\n",
        "        self.time_embed_dim = time_embed_dim\n",
        "        self.time_embed = nn.Sequential(\n",
        "            SinusoidalPositionEmbeddings(time_embed_dim),\n",
        "            nn.Linear(time_embed_dim, model_dim),\n",
        "            Swish(),\n",
        "            nn.Linear(model_dim, model_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, time):\n",
        "        return self.time_embed(time)\n",
        "\n",
        "class LabelEmbedding(nn.Module):\n",
        "    \"\"\"Label embedding with MLP and Swish activation\"\"\"\n",
        "    def __init__(self, num_classes, embed_dim, model_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_classes, embed_dim)\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(embed_dim, model_dim),\n",
        "            Swish(),\n",
        "            nn.Linear(model_dim, model_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, labels):\n",
        "        x = self.embedding(labels)\n",
        "        return self.projection(x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# 4.2 Time-dependent Multi-head Self-Attention (TMSA)\n",
        "\n",
        "class TimeDependentMultiHeadAttention(nn.Module):\n",
        "    \"\"\"Time-dependent Multi-head Self-Attention (TMSA)\"\"\"\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.0):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Spatial projection weights (Wqs, Wks, Wvs)\n",
        "        self.to_q_spatial = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_k_spatial = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_v_spatial = nn.Linear(dim, inner_dim, bias=False)\n",
        "\n",
        "        # Temporal projection weights (Wqt, Wkt, Wvt)\n",
        "        self.to_q_temporal = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_k_temporal = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_v_temporal = nn.Linear(dim, inner_dim, bias=False)\n",
        "\n",
        "        # Output projection\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Relative position bias\n",
        "        self.rel_pos_bias = nn.Parameter(torch.zeros(heads, 49, 49))\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        \"\"\"Forward pass of TMSA\"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        h = self.heads\n",
        "\n",
        "        # Spatial components\n",
        "        q_spatial = self.to_q_spatial(x).reshape(batch_size, seq_len, h, -1).permute(0, 2, 1, 3)\n",
        "        k_spatial = self.to_k_spatial(x).reshape(batch_size, seq_len, h, -1).permute(0, 2, 1, 3)\n",
        "        v_spatial = self.to_v_spatial(x).reshape(batch_size, seq_len, h, -1).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Temporal components\n",
        "        time_emb_expanded = time_emb.unsqueeze(1)\n",
        "        q_temporal = self.to_q_temporal(time_emb_expanded).reshape(batch_size, 1, h, -1).permute(0, 2, 1, 3)\n",
        "        k_temporal = self.to_k_temporal(time_emb_expanded).reshape(batch_size, 1, h, -1).permute(0, 2, 1, 3)\n",
        "        v_temporal = self.to_v_temporal(time_emb_expanded).reshape(batch_size, 1, h, -1).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Broadcast temporal components\n",
        "        q_temporal = q_temporal.expand(-1, -1, seq_len, -1)\n",
        "        k_temporal = k_temporal.expand(-1, -1, seq_len, -1)\n",
        "        v_temporal = v_temporal.expand(-1, -1, seq_len, -1)\n",
        "\n",
        "        # Combine spatial and temporal components\n",
        "        q = q_spatial + q_temporal\n",
        "        k = k_spatial + k_temporal\n",
        "        v = v_spatial + v_temporal\n",
        "\n",
        "        # Attention calculation\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
        "\n",
        "        # Add relative position bias\n",
        "        if seq_len <= 49:\n",
        "            bias = self.rel_pos_bias[:, :seq_len, :seq_len]\n",
        "            dots = dots + bias.unsqueeze(0)\n",
        "\n",
        "        attn = self.attend(dots)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.permute(0, 2, 1, 3).reshape(batch_size, seq_len, -1)\n",
        "\n",
        "        return self.to_out(out)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# 4.3 Feed Forward Network\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"MLP with time conditioning\"\"\"\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.0):\n",
        "        super().__init__()\n",
        "        # MLP for spatial features\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            Swish(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # MLP for time conditioning\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            Swish(),\n",
        "            nn.Linear(hidden_dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        # Time conditioning\n",
        "        time_out = self.time_mlp(time_emb).unsqueeze(1)\n",
        "        return self.net(x) + time_out  # Additive conditioning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# 4.4 Latent DiffiT Transformer Block\n",
        "\n",
        "class LatentDiffiTTransformerBlock(nn.Module):\n",
        "    \"\"\"LatentDiffiT Transformer Block with TMSA and time-conditioned FFN\"\"\"\n",
        "    def __init__(self, dim, heads=8, dim_head=64, mlp_dim=None, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = TimeDependentMultiHeadAttention(dim, heads=heads, dim_head=dim_head, dropout=dropout)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        mlp_dim = mlp_dim or (dim * 4)\n",
        "        self.mlp = FeedForward(dim, mlp_dim, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, time_emb):\n",
        "        # LayerNorm and TMSA with residual connection\n",
        "        x = x + self.attn(self.norm1(x), time_emb)\n",
        "        # LayerNorm and MLP with residual connection\n",
        "        x = x + self.mlp(self.norm2(x), time_emb)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# 4.5 Encoder class for patching latent representations\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encoder for patching latent representations\"\"\"\n",
        "    def __init__(self, img_size=256, patch_size=16, hidden_dim=768):\n",
        "        super().__init__()\n",
        "        \n",
        "        latent_channels = 4\n",
        "        latent_size = img_size // 8  # 32x32 for 256x256 images\n",
        "        \n",
        "        self.patch_size = patch_size\n",
        "        self.latent_size = latent_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # Ensure latent_size is divisible by patch_size\n",
        "        assert latent_size % patch_size == 0, \"latent_size must be divisible by patch_size\"\n",
        "        self.patches_per_side = latent_size // patch_size\n",
        "        self.num_patches = self.patches_per_side ** 2\n",
        "        \n",
        "        # Patch embedding layer (similar to ViT)\n",
        "        self.patch_embedding = nn.Conv2d(\n",
        "            in_channels=latent_channels,\n",
        "            out_channels=hidden_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "        \n",
        "        # Position embedding\n",
        "        self.position_embedding = nn.Parameter(torch.zeros(1, self.num_patches, hidden_dim))\n",
        "        \n",
        "    def forward(self, latents):\n",
        "        patches = self.patch_embedding(latents)  # [B, hidden_dim, patches_per_side, patches_per_side]\n",
        "        \n",
        "        # Reshape and add positional embedding\n",
        "        embedded = rearrange(patches, 'b c h w -> b (h w) c')\n",
        "        embedded = embedded + self.position_embedding\n",
        "        \n",
        "        return embedded  # [B, num_patches, hidden_dim]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# 4.6 Unpatchify to convert from sequence back to grid format\n",
        "\n",
        "class Unpatchify(nn.Module):\n",
        "    \"\"\"Convert patch sequence back to grid format\"\"\"\n",
        "    def __init__(self, patch_size, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, L, C) - batch size, number of patches, channels\n",
        "        return: (B, C, H, W) - grid with hidden_dim channels\n",
        "        \"\"\"\n",
        "        B, L, C = x.shape\n",
        "        assert C == self.hidden_dim, f\"Input channels must be {self.hidden_dim}, got {C}\"\n",
        "        patches_per_side = int(math.sqrt(L))\n",
        "        H = W = patches_per_side * self.patch_size\n",
        "        \n",
        "        # Reshape from sequence to grid\n",
        "        x = x.reshape(B, patches_per_side, patches_per_side, C)\n",
        "        \n",
        "        # Convert from [B, patches_per_side, patches_per_side, C] to [B, C, H, W]\n",
        "        x = x.permute(0, 3, 1, 2)  # [B, C, patches_per_side, patches_per_side]\n",
        "        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=False)  # Upsample to [B, C, H, W]\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# 4.7 Decoder to predict noise\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"\"\"Decoder to predict noise in latent space\"\"\"\n",
        "    def __init__(self, in_channels, hidden_dim, out_channels=4):\n",
        "        super().__init__()\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_dim, kernel_size=3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm2d(hidden_dim),\n",
        "            nn.Conv2d(hidden_dim, out_channels, kernel_size=3, padding=1),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, C, H, W) - input from unpatchify, C = hidden_dim\n",
        "        return: (B, 4, H, W) - predicted noise in latent space\n",
        "        \"\"\"\n",
        "        return self.decoder(x)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# 4.8 Latent DiffiT Transformer\n",
        "\n",
        "class LatentDiffiTTransformer(nn.Module):\n",
        "    \"\"\"Latent DiffiT Transformer for diffusion in latent space\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        heads=8,\n",
        "        dim_head=64,\n",
        "        mlp_dim=None,\n",
        "        dropout=0.0,\n",
        "        time_embed_dim=None,\n",
        "        label_embed_dim=None,\n",
        "        num_classes=1000\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Parameters\n",
        "        self.dim = dim\n",
        "        time_embed_dim = time_embed_dim or dim * 4\n",
        "        label_embed_dim = label_embed_dim or dim\n",
        "        \n",
        "        # Time and Label Embeddings\n",
        "        self.time_embedding = TimeEmbedding(time_embed_dim, dim)\n",
        "        self.label_embedding = LabelEmbedding(num_classes, label_embed_dim, dim)\n",
        "        \n",
        "        # Transformer blocks\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            LatentDiffiTTransformerBlock(\n",
        "                dim=dim,\n",
        "                heads=heads,\n",
        "                dim_head=dim_head,\n",
        "                mlp_dim=mlp_dim,\n",
        "                dropout=dropout\n",
        "            ) for _ in range(depth)\n",
        "        ])\n",
        "        \n",
        "        # Final layer norm\n",
        "        self.final_norm = nn.LayerNorm(dim)\n",
        "        \n",
        "    def combine_embeddings(self, time_emb, label_emb=None):\n",
        "        # Combine time and label embeddings\n",
        "        if label_emb is not None:\n",
        "            combined_emb = time_emb + label_emb\n",
        "        else:\n",
        "            combined_emb = time_emb\n",
        "        return combined_emb\n",
        "    \n",
        "    def forward(self, x, time, labels=None):\n",
        "        # Create time token from timestep\n",
        "        time_emb = self.time_embedding(time)\n",
        "        \n",
        "        # Create and combine with label embedding if provided\n",
        "        if labels is not None:\n",
        "            label_emb = self.label_embedding(labels)\n",
        "            combined_emb = self.combine_embeddings(time_emb, label_emb)\n",
        "        else:\n",
        "            combined_emb = time_emb\n",
        "        \n",
        "        # Process through transformer blocks\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, combined_emb)\n",
        "        \n",
        "        # Final layer norm\n",
        "        x = self.final_norm(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# 4.9 Full LatentDiffiT Pipeline\n",
        "\n",
        "class LatentDiffiTPipeline(nn.Module):\n",
        "    \"\"\"Complete LatentDiffiT Pipeline for training and sampling\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=256,\n",
        "        patch_size=16,\n",
        "        hidden_dim=768,\n",
        "        depth=12,\n",
        "        heads=12,\n",
        "        dim_head=64,\n",
        "        mlp_dim=None,\n",
        "        dropout=0.0,\n",
        "        time_embed_dim=None,\n",
        "        label_embed_dim=None,\n",
        "        num_classes=1000\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        if mlp_dim is None:\n",
        "            mlp_dim = hidden_dim * 4\n",
        "        \n",
        "        # Initialize components\n",
        "        self.encoder = Encoder(\n",
        "            img_size=img_size,\n",
        "            patch_size=patch_size,\n",
        "            hidden_dim=hidden_dim\n",
        "        )\n",
        "        \n",
        "        self.transformer = LatentDiffiTTransformer(\n",
        "            dim=hidden_dim,\n",
        "            depth=depth,\n",
        "            heads=heads,\n",
        "            dim_head=dim_head,\n",
        "            mlp_dim=mlp_dim,\n",
        "            dropout=dropout,\n",
        "            time_embed_dim=time_embed_dim,\n",
        "            label_embed_dim=label_embed_dim,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "        \n",
        "        self.unpatchify = Unpatchify(patch_size=patch_size, hidden_dim=hidden_dim)\n",
        "        \n",
        "        self.decoder = Decoder(\n",
        "            in_channels=hidden_dim,\n",
        "            hidden_dim=hidden_dim // 2,\n",
        "            out_channels=4  # Match latent space channels\n",
        "        )\n",
        "    \n",
        "    def forward(self, noisy_latents, timesteps, labels=None):\n",
        "        # Apply classifier-free guidance to the first three channels\n",
        "        noisy_latents[:, :3, :, :] *= (1 + noisy_latents[:, :3, :, :])\n",
        "        \n",
        "        # Process through encoder to get patches\n",
        "        embedded = self.encoder.patch_embedding(noisy_latents)\n",
        "        embedded = rearrange(embedded, 'b c h w -> b (h w) c') + self.encoder.position_embedding\n",
        "        \n",
        "        # Process through transformer\n",
        "        transformer_output = self.transformer(embedded, timesteps, labels)\n",
        "        \n",
        "        # Convert back to spatial representation\n",
        "        unpatched = self.unpatchify(transformer_output)\n",
        "        \n",
        "        # Predict noise\n",
        "        predicted_noise = self.decoder(unpatched)\n",
        "        \n",
        "        return predicted_noise\n",
        "    \n",
        "    def sample(self, num_samples, timesteps, device, labels=None):\n",
        "        \"\"\"Generate images using the diffusion process\"\"\"\n",
        "        latent_size = self.img_size // 8\n",
        "        latents = torch.randn(num_samples, 4, latent_size, latent_size).to(device)\n",
        "        timesteps_tensor = torch.arange(timesteps - 1, -1, -1, device=device).float()\n",
        "        \n",
        "        if labels is None:\n",
        "            labels = torch.randint(0, self.num_classes, (num_samples,), device=device)\n",
        "        \n",
        "        # Diffusion sampling loop\n",
        "        for t in timesteps_tensor:\n",
        "            t_batch = t.repeat(num_samples).float()\n",
        "            predicted_noise = self.forward(latents, t_batch, labels)\n",
        "            \n",
        "            # Simple DDPM update rule\n",
        "            alpha = 1 - t / timesteps\n",
        "            latents = (latents - (1 - alpha) * predicted_noise) / alpha\n",
        "        \n",
        "        # Decode latents to images\n",
        "        with torch.no_grad():\n",
        "            images = decode_from_latent(latents)\n",
        "        \n",
        "        return images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 5. Initialize Model\n",
        "\n",
        "Create the LatentDiffiT model with appropriate configurations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Initialize the LatentDiffiT pipeline\n",
        "model = LatentDiffiTPipeline(\n",
        "    img_size=256,\n",
        "    patch_size=16,\n",
        "    hidden_dim=768,\n",
        "    depth=6,       # Reduced for faster training\n",
        "    heads=8,\n",
        "    dim_head=64,\n",
        "    num_classes=num_classes\n",
        ").to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "print(f\"Number of transformer blocks: {len(model.transformer.transformer_blocks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 6. Training Function\n",
        "\n",
        "Define the training loop for LatentDiffiT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "def train_diffit(model, dataloader, num_epochs, num_timesteps, device, learning_rate, save_dir=MODEL_DIR, output_dir=OUTPUT_DIR):\n",
        "    \"\"\"Train the LatentDiffiT model\"\"\"\n",
        "    # Setup optimizer and loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "    \n",
        "    # Set up directories\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Training history\n",
        "    history = {'epoch': [], 'loss': []}\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        \n",
        "        for batch_idx, (images, labels) in enumerate(progress_bar):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = images.shape[0]\n",
        "            \n",
        "            # Create random timesteps\n",
        "            timesteps = torch.randint(0, num_timesteps, (batch_size,), device=device).float()\n",
        "            \n",
        "            # Encode images to latent space\n",
        "            with torch.no_grad():\n",
        "                latents = encode_to_latent(images)  # [B, 4, H/8, W/8]\n",
        "            \n",
        "            # Add noise to latent space (simple DDPM)\n",
        "            noise = torch.randn_like(latents)\n",
        "            t = timesteps / num_timesteps\n",
        "            noisy_latents = (1 - t.view(-1, 1, 1, 1)) * latents + t.view(-1, 1, 1, 1) * noise\n",
        "            \n",
        "            # Predict noise in latent space\n",
        "            optimizer.zero_grad()\n",
        "            predicted_noise = model(noisy_latents, timesteps, labels)  # [B, 4, H/8, W/8]\n",
        "            loss = criterion(predicted_noise, noise)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Update progress bar\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "            \n",
        "            # Save intermediate results for long training runs\n",
        "            if batch_idx % 500 == 0 and batch_idx > 0:\n",
        "                print(f\"\\nSaving intermediate model at epoch {epoch+1}, batch {batch_idx}\")\n",
        "                intermediate_path = os.path.join(save_dir, f\"latent_diffit_epoch{epoch+1}_batch{batch_idx}.pth\")\n",
        "                torch.save(model.state_dict(), intermediate_path)\n",
        "        \n",
        "        # Calculate and log average loss\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
        "        \n",
        "        # Update history\n",
        "        history['epoch'].append(epoch + 1)\n",
        "        history['loss'].append(avg_loss)\n",
        "        \n",
        "        # Save model checkpoint\n",
        "        checkpoint_path = os.path.join(save_dir, f\"latent_diffit_epoch_{epoch+1}.pth\")\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f\"Model saved to {checkpoint_path}\")\n",
        "        \n",
        "        # Generate samples\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Generate 16 samples\n",
        "            num_samples = 16\n",
        "            print(f\"Generating {num_samples} sample images...\")\n",
        "            generated_images = model.sample(num_samples=num_samples, timesteps=50, device=device)\n",
        "            \n",
        "            # Convert from [-1, 1] to [0, 1] for saving\n",
        "            generated_images = (generated_images + 1) / 2\n",
        "            \n",
        "            # Save images\n",
        "            output_path = os.path.join(output_dir, f\"epoch_{epoch+1}.png\")\n",
        "            vutils.save_image(generated_images, output_path, nrow=4, padding=2)\n",
        "            print(f\"Generated images saved to {output_path}\")\n",
        "    \n",
        "    # Plot loss history\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(history['epoch'], history['loss'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.savefig(os.path.join(save_dir, 'training_loss.png'))\n",
        "    plt.show()\n",
        "    \n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 7. Train the Model\n",
        "\n",
        "Run the training process for the LatentDiffiT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Training parameters\n",
        "num_epochs = 10\n",
        "num_timesteps = 1000\n",
        "learning_rate = 3e-5\n",
        "\n",
        "# Start training\n",
        "print(f\"Starting training for {num_epochs} epochs...\")\n",
        "history = train_diffit(\n",
        "    model=model,\n",
        "    dataloader=dataloader,\n",
        "    num_epochs=num_epochs,\n",
        "    num_timesteps=num_timesteps,\n",
        "    device=device,\n",
        "    learning_rate=learning_rate\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 8. Generate Images\n",
        "\n",
        "Load the trained model and generate images with different class labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "def generate_images(model, device, num_images=16, timesteps=50, class_ids=None, output_dir=OUTPUT_DIR):\n",
        "    \"\"\"Generate images using the trained model\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Prepare class labels if provided\n",
        "    if class_ids is not None:\n",
        "        # Convert to tensor and ensure it's on the correct device\n",
        "        labels = torch.tensor(class_ids, device=device)\n",
        "        print(f\"Generating images for specific classes: {class_ids}\")\n",
        "        \n",
        "        # Make sure we have enough labels\n",
        "        if len(labels) < num_images:\n",
        "            labels = labels.repeat((num_images + len(labels) - 1) // len(labels))\n",
        "            labels = labels[:num_images]\n",
        "    else:\n",
        "        # Generate random class labels\n",
        "        labels = torch.randint(0, model.num_classes, (num_images,), device=device)\n",
        "        print(\"Generating images with random class labels\")\n",
        "    \n",
        "    # Generate images\n",
        "    print(f\"Generating {num_images} images with {timesteps} timesteps...\")\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        generated_images = model.sample(num_samples=num_images, timesteps=timesteps, device=device, labels=labels)\n",
        "        end_time = time.time()\n",
        "    \n",
        "    # Process and save images\n",
        "    generated_images = (generated_images + 1) / 2  # Convert from [-1, 1] to [0, 1]\n",
        "    \n",
        "    # Save as grid\n",
        "    grid_path = os.path.join(output_dir, f\"generated_grid_{time.strftime('%Y%m%d_%H%M%S')}.png\")\n",
        "    vutils.save_image(generated_images, grid_path, nrow=int(math.sqrt(num_images)), padding=2)\n",
        "    \n",
        "    # Save individual images\n",
        "    for i, img in enumerate(generated_images):\n",
        "        img_path = os.path.join(output_dir, f\"generated_{i}_class{labels[i].item()}.png\")\n",
        "        vutils.save_image(img, img_path)\n",
        "    \n",
        "    print(f\"Generation complete in {end_time - start_time:.2f} seconds\")\n",
        "    print(f\"Images saved to {output_dir}\")\n",
        "    \n",
        "    # Display the grid\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    grid = vutils.make_grid(generated_images, nrow=int(math.sqrt(num_images)), padding=2, normalize=False)\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Generated Images\")\n",
        "    plt.show()\n",
        "    \n",
        "    return generated_images\n",
        "\n",
        "# Load the best model (usually the last epoch or you can pick a specific one)\n",
        "best_model_path = os.path.join(MODEL_DIR, f\"latent_diffit_epoch_{num_epochs}.pth\")\n",
        "if os.path.exists(best_model_path):\n",
        "    print(f\"Loading best model from {best_model_path}\")\n",
        "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "else:\n",
        "    print(\"Best model not found, using the current model state.\")\n",
        "\n",
        "# Generate images with the trained model\n",
        "import time  # Import time for timestamping\n",
        "\n",
        "# Generate 16 random images\n",
        "generate_images(model, device, num_images=16, timesteps=100)\n",
        "\n",
        "# Generate specific classes (if you know the class indices)\n",
        "# For example, generate dog breeds if your dataset contains them\n",
        "specific_classes = [15, 97, 182, 344]  # Example class indices\n",
        "generate_images(model, device, num_images=4, timesteps=100, class_ids=specific_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 9. Interactive Image Generation Tool\n",
        "\n",
        "Create an interactive tool to generate images with different parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": [
        "# Only run in interactive environments like Colab\n",
        "try:\n",
        "    from ipywidgets import interact, IntSlider, FloatSlider, Dropdown, Text\n",
        "    import ipywidgets as widgets\n",
        "    \n",
        "    # Get available classes\n",
        "    class_names = sorted(dataset.classes)\n",
        "    class_dict = {name: idx for idx, name in enumerate(class_names)}\n",
        "    \n",
        "    # Function for interactive image generation\n",
        "    def interactive_generate(num_images=4, timesteps=100, class_name=class_names[0], seed=42):\n",
        "        # Set random seed\n",
        "        torch.manual_seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "        \n",
        "        # Get class index\n",
        "        class_idx = class_dict[class_name]\n",
        "        print(f\"Generating {num_images} images of class '{class_name}' (index: {class_idx})\")\n",
        "        \n",
        "        # Generate images\n",
        "        return generate_images(model, device, num_images=num_images, timesteps=timesteps, \n",
        "                              class_ids=[class_idx] * num_images)\n",
        "    \n",
        "    # Create interactive widgets\n",
        "    interact(\n",
        "        interactive_generate,\n",
        "        num_images=IntSlider(min=1, max=16, step=1, value=4, description=\"Images:\"),\n",
        "        timesteps=IntSlider(min=10, max=200, step=10, value=100, description=\"Steps:\"),\n",
        "        class_name=Dropdown(options=class_names[:100], description=\"Class:\"),  # Limiting to 100 for performance\n",
        "        seed=IntSlider(min=0, max=1000, step=1, value=42, description=\"Seed:\")\n",
        "    )\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"Interactive widgets not available. Skipping interactive tool.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 10. Conclusion\n",
        "\n",
        "The LatentDiffiT model combines transformers with diffusion models to operate in the latent space of a pre-trained VAE. This approach allows for efficient training and generation of high-quality images.\n",
        "\n",
        "Key components of the implementation:\n",
        "\n",
        "1. **VAE Integration**: Using a pre-trained Stable Diffusion VAE to work in latent space\n",
        "2. **Transformer Architecture**: Implementing the Time-dependent Multi-head Self-Attention (TMSA) mechanism\n",
        "3. **Diffusion Process**: Training the model to predict noise in the latent space\n",
        "4. **Class-Conditional Generation**: Support for generating images conditioned on class labels\n",
        "\n",
        "The notebook provides a complete pipeline from data loading to training and image generation. You can experiment with different hyperparameters to optimize the quality of generated images."
      ]
    }
  ]
}